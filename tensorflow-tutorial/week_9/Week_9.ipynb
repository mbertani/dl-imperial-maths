{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 9: Normalising flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tensorflow bijectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros([2], tf.float32), scale_diag=tf.constant([1, 1], tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_BATCH_SIZE = 512\n",
    "tf.set_random_seed(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = base_dist.sample(SAMPLE_BATCH_SIZE)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_samples = z.eval()\n",
    "print(type(z_samples))\n",
    "print(z_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(z_samples[:, 0], z_samples[:, 1], s=10)\n",
    "plt.title(\"Base distribution: standard normal\")\n",
    "plt.xlim([-4, 4])\n",
    "plt.ylim([-4, 4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bijector is used to transform distributions. Bijectors are the building blocks for a normalising flow. \n",
    "They are characterised by the following three main methods:\n",
    "    1. forward\n",
    "    2. inverse\n",
    "    3. log_det_jacobian\n",
    "\n",
    "Conventionally, think of the `forward` operation as acting on the base distribution (generate samples) and the `inverse` operation is used to calculate probabilities.\n",
    "\n",
    "For example, the Affine Bijector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "affine_bijector = tfb.Affine(shift=[1., -1.], scale_diag=[0.5, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fwd_z = affine_bijector.forward(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_samples, x_samples = sess.run([z, fwd_z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "ax.scatter(z_samples[:, 0], z_samples[:, 1], s=10)\n",
    "ax.set_title(\"Base distribution: standard normal\")\n",
    "ax.set_xlim([-5, 5])\n",
    "ax.set_ylim([-5, 5])\n",
    "\n",
    "ax2.scatter(x_samples[:, 0], x_samples[:, 1], s=10, color='r')\n",
    "ax2.set_title(\"Transformed distribution: shift [1, -1], scale [0.5, 1.5]\")\n",
    "ax2.set_xlim([-5, 5])\n",
    "ax2.set_ylim([-5, 5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fwd_inv_z = affine_bijector.inverse(fwd_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = np.random.random((SAMPLE_BATCH_SIZE, 2))\n",
    "print(np.allclose(latents, sess.run(fwd_inv_z, feed_dict={z: latents})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(shape=(1, 2), dtype=tf.float32)\n",
    "\n",
    "log_det_dzdx = affine_bijector.inverse_log_det_jacobian(x, event_ndims=1)\n",
    "log_det_dzdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_x = affine_bijector.inverse(x)\n",
    "inv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob_inv_x = base_dist.log_prob(inv_x)\n",
    "log_prob_inv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fixed_sample = np.array([[1., -1.]])  # Mode of the transformed distribution\n",
    "\n",
    "sess.run(log_det_dzdx, feed_dict={x: x_fixed_sample})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: Jacobian determinant is just the product of scaling factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- np.log(0.5) - np.log(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate log probability of `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(log_prob_inv_x + log_det_dzdx, feed_dict={x: np.array([[1., -1.]])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(np.sqrt(1 / (2 * np.pi)**2)) - np.log(0.5) - np.log(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned flow example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_dist = tfd.Normal(loc=0., scale=4.)\n",
    "x2 = x2_dist.sample(SAMPLE_BATCH_SIZE)\n",
    "x1_dist = tfd.Normal(loc=.25 * tf.square(x2), scale=tf.ones(SAMPLE_BATCH_SIZE, dtype=tf.float32))\n",
    "x1 = x1_dist.sample()\n",
    "x = tf.stack([x1, x2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_samples = sess.run(x)\n",
    "plt.scatter(np_samples[:, 0], np_samples[:, 1], s=10)\n",
    "plt.xlim([-5, 30])\n",
    "plt.ylim([-10, 10])\n",
    "plt.title(\"Target distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the normalising flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(tfb.Bijector):\n",
    "    def __init__(self, alpha=0.5, validate_args=False, name=\"leaky_relu\"):\n",
    "        super().__init__(forward_min_event_ndims=1, validate_args=validate_args, name=name)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _forward(self, x):\n",
    "        return tf.where(tf.greater_equal(x, 0), x, self.alpha * x)\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        return tf.where(tf.greater_equal(y, 0), y, 1. / self.alpha * y)\n",
    "\n",
    "    def _inverse_log_det_jacobian(self, y):\n",
    "        I = tf.ones_like(y)\n",
    "        J_inv = tf.where(tf.greater_equal(y, 0), I, 1.0 / self.alpha * I)\n",
    "        log_abs_det_J_inv = tf.log(tf.abs(J_inv))\n",
    "        return tf.reduce_sum(log_abs_det_J_inv, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the Bijector layers that will make up our normalising flow\n",
    "bijectors_list = []\n",
    "num_layers = 6\n",
    "d, r = 2, 2\n",
    "\n",
    "for i in range(num_layers):\n",
    "    with tf.variable_scope('bijector_%d' % i):\n",
    "        V = tf.get_variable('V', [d, r], dtype=tf.float32)  # factor loading\n",
    "        shift = tf.get_variable('shift', [d], dtype=tf.float32)  # affine shift\n",
    "        L = tf.get_variable('L', [d * (d + 1) / 2],\n",
    "                            dtype=tf.float32)  # lower triangular\n",
    "        bijectors_list.append(tfb.Affine(\n",
    "            scale_tril=tfd.fill_triangular(L),\n",
    "            scale_perturb_factor=V,\n",
    "            shift=shift,\n",
    "            name=\"affine_{}\".format(i)\n",
    "        ))\n",
    "        \n",
    "        if i != num_layers - 1:\n",
    "            alpha = tf.abs(tf.get_variable('alpha', [], dtype=tf.float32)) + .01\n",
    "            bijectors_list.append(LeakyReLU(alpha=alpha, name=\"leaky_relu_{}\".format(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the network from the list of Bijectors.\n",
    "\n",
    "Note that `tfb.Chain` processes a list of bijectors in the reverse order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_bijector = tfb.Chain(list(reversed(bijectors_list)), name='mlp_bijector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `TransformedDistribution` takes a base distribution and a bijector (the `Chain` above) to define the flow. \n",
    "\n",
    "We will use the same base distribution defined earlier (multivariate normal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flow = tfd.TransformedDistribution(distribution=base_dist, bijector=mlp_bijector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the layer transformations before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = base_dist.sample(SAMPLE_BATCH_SIZE)\n",
    "samples = [z]  # Collect samples for each layer in the network\n",
    "names = [base_dist.name]  # Collect the names for each layer in the network\n",
    "h = z\n",
    "for bijector in reversed(flow.bijector.bijectors):\n",
    "    h = bijector.forward(h)\n",
    "    samples.append(h)\n",
    "    names.append(bijector.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_samples = sess.run(samples)\n",
    "num_layers = len(layer_samples)  # 12\n",
    "num_plot_rows = 3\n",
    "f, arr = plt.subplots(num_plot_rows, num_layers // num_plot_rows, figsize=(15, num_plot_rows * 3))\n",
    "\n",
    "for i in range(num_plot_rows):\n",
    "    for j in range(num_layers // num_plot_rows):\n",
    "        layer_num = (i * (num_layers // num_plot_rows)) + j\n",
    "        \n",
    "        X = layer_samples[layer_num]\n",
    "        arr[i, j].scatter(X[:, 0], X[:, 1], s=8)\n",
    "        arr[i, j].set_xlim([-5, 5])\n",
    "        arr[i, j].set_ylim([-5, 5])\n",
    "        arr[i, j].set_title(names[layer_num])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_mean(flow.log_prob(x))\n",
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train a new model\n",
    "\n",
    "# import time\n",
    "\n",
    "# # Need to initialise the optimizer variables\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "# NUM_STEPS = int(50000)\n",
    "# global_step = []\n",
    "# np_losses = []\n",
    "# start_time = time.time()\n",
    "# for i in range(NUM_STEPS):\n",
    "#     _, np_loss = sess.run([train_op, loss])\n",
    "#     if i % 1000 == 0:\n",
    "#         global_step.append(i)\n",
    "#         np_losses.append(np_loss)\n",
    "#     if i % int(1e4) == 0:\n",
    "#         print(i, np_loss)\n",
    "# end_time = time.time()\n",
    "# saver.save(sess, './', global_step=global_step[-1])\n",
    "# np.save('./np_losses.npy', np_losses)\n",
    "# np.save('./global_step.npy', global_step)\n",
    "# print(\"Training time: {}\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a previously saved model\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "meta_graph = tf.train.latest_checkpoint('./')\n",
    "saver.restore(sess, meta_graph)\n",
    "np_losses = np.load('./np_losses.npy')\n",
    "global_step = np.load('./global_step.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 10\n",
    "plt.plot(global_step[start:], np_losses[start:])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_samples = sess.run(samples)\n",
    "num_plot_rows = 3\n",
    "num_layers = len(layer_samples)  # 12\n",
    "f, arr = plt.subplots(num_plot_rows, num_layers // num_plot_rows, figsize=(15, num_plot_rows * 3))\n",
    "\n",
    "for i in range(num_plot_rows):\n",
    "    for j in range(num_layers // num_plot_rows):\n",
    "        layer_num = (i * (num_layers // num_plot_rows)) + j\n",
    "        \n",
    "        X = layer_samples[layer_num]\n",
    "        arr[i, j].scatter(X[:, 0], X[:, 1], s=8)\n",
    "        arr[i, j].set_xlim([-5, 20])\n",
    "        arr[i, j].set_ylim([-10, 10])\n",
    "        arr[i, j].set_title(names[layer_num])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [z, mlp_bijector.forward(z)]\n",
    "layer_samples = sess.run(samples)\n",
    "f, arr = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "X = layer_samples[0]\n",
    "arr[0].scatter(X[:, 0], X[:, 1], s=8)\n",
    "arr[0].set_xlim([-5, 5])\n",
    "arr[0].set_ylim([-5, 5])\n",
    "arr[0].set_title('Base distribution')\n",
    "X = layer_samples[1]\n",
    "arr[1].scatter(X[:, 0], X[:, 1], s=8, color='red')\n",
    "arr[1].set_xlim([-5, 30])\n",
    "arr[1].set_ylim([-10, 10])\n",
    "arr[1].set_title('Transformed distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
